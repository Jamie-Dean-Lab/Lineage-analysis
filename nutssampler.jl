using Printf
using Dates
using Plots

Base.@kwdef mutable struct Nutsoptions
    # options for nutssampler

    autogenerated::Bool = true                      # indicate if autogenerated
    name::String = ""                               # name/comment
    noparams::UInt64 = 1                            # size of parameter space
    without::UInt64 = 1                             # '0' for only warnings, '3' for debugging
    withgraphical::Bool = false                     # 'true' for graphical output in the end, 'false' otherwise
    burnin::UInt64 = 10000
    MCmax::UInt64 = 2*burnin
    subsamplefreq::UInt64 = 1                       # number of subsamples per recorded sample
    approxgradient::Bool = true                     # 'true', if use Euler approximation for gradient estimation, 'false' for using gradient function
    dx::Array{Float64,1} = ones(noparams).*(1E-7)   # should be positive
    timestep::Float64 = 1E-2                        # should be positive
    mass::Float64 = 1.0                             # should be positive
    Delta_max::Float64 = 1E3                        # should be non-negative and large
    keephistoryof::Array{Bool,1} = trues(noparams)  # 'true', if respective parameter should be recorded, 'false' otherwise
    alpha::Float64 = 0.0
    n_alpha::UInt64 = 0
    # stepsize adjustment:
    adjusttimestep::Bool = true
    timeadj_reasonableacceptancerate::Float64 = 0.65
    timeadj_Hb::Float64 = 0.0                       # measures deviations from desired rejection rate
    timeadj_t0::Float64 = 10.0                      # suppress early iterations
    timeadj_gamma::Float64 = 0.05                   # scaling for the penalty - larger changes for smaller gamma
    timeadj_kappa::Float64 = 0.75                   # exponent, how quickly timestepcorrection changes fade out in the MC evolution
    timeadj_mu::Float64 = log(10*timestep)          # offset for timestep
    timeadj_timestepb::Float64 = 1.0
    rejrate::Float64 = 0.0                          # rejection rate
end     # end of Nutsoptions struct

function nutssampler( logtarget::Function, gradient::Function, x_init::Array{Float64,1}, nutsopt::Nutsoptions )
    # nutssampler

    t1 = DateTime(now())
    # initialise nutsopt:
    if( nutsopt.autogenerated )                     # if input is autogenerated
        noparams = length(x_init)
        dx = ones(noparams)*nutsopt.dx[1]
        keephistoryof = trues(noparams)
        nutsopt.noparams = noparams;    nutsopt.dx = dx;    nutsopt.keephistoryof = keephistoryof   # alpha, n_alpha will get overwritten at tree of depth 0
    end     # end if nutsoptions not given
    x_curr = deepcopy( x_init )                     # start location
    logtarget_curr = getlogtarget_nuts( logtarget, x_init, nutsopt )
    gradx = getgradient_nuts( logtarget,gradient, x_init, nutsopt )
    if( isinf(logtarget_curr) | isnan(logtarget_curr) | !all(.!isinf.(gradx)) | !all(.!isnan.(gradx)) )
        @printf( " (%s) Warning - nutssampler: Pathological initial condition, logtarget = %+1.5e, gradx = [ %s ], x_init = [ %s ].\n", nutsopt.name, logtarget_curr, getstringfromvector(gradx), getstringfromvector(x_init) )
    end     # end if pathological initial condition
    nextprogessinfo = 1                 # first information bit to be next
    x_hist = zeros(sum(nutsopt.keephistoryof),nutsopt.MCmax+1); x_hist[:,1] = x_curr[nutsopt.keephistoryof] # initialise history
    accrate_hist = zeros(nutsopt.MCmax+1)
    for j_MC = 2:(nutsopt.MCmax+1)
        if( (nutsopt.without>=1) & ((nextprogessinfo*0.1)<=(j_MC/(nutsopt.MCmax+1))) )
            @printf( " (%s) Info - nutssampler: Start iteration %d now (after %1.3f sec).\n", nutsopt.name, j_MC, (DateTime(now())-t1)/Millisecond(1000) )
            nextprogessinfo += 1        # progess to next step of information
        end     # end if time for output
        for j_sumbsample = 1:nutsopt.subsamplefreq
            (x_curr,nutsopt) = singlenutsupdate( logtarget,gradient, x_curr, nutsopt )
            x_hist[:,j_MC] = x_curr[nutsopt.keephistoryof]
        end     # end of subsample loop
        accrate_hist[j_MC] = nutsopt.alpha/nutsopt.n_alpha
        nutsopt = adjusttimestep( j_MC,nutsopt )
    end     # end of MC iterations
    statsrange = (nutsopt.burnin+1):(nutsopt.MCmax+1)           # post-burnin samples for statistical evaluation
    nutsopt.rejrate = 1 - mean(accrate_hist[statsrange])        # mean rejection rate

    # output:
    # ...graphical:
    if( nutsopt.withgraphical )
        res = Int64(ceil( 2*((length(statsrange))^(1/3)) ))
        jj_par = 0                  # initialise
        for j_par = collect(1:nutsopt.noparams)[nutsopt.keephistoryof]
            jj_par += 1
            p1 = plot( 1:(nutsopt.MCmax+1), x_hist[jj_par,:], title=@sprintf("(%s) par %d evolution",nutsopt.name, j_par), xlabel="MCit",ylabel=@sprintf("par %d",j_par) )
            display(p1)
            p2 = histogram( x_hist[jj_par,statsrange], nbins=res, title=@sprintf("(%s) par %d histogram after burnin",nutsopt.name, j_par), xlabel=@sprintf("par %d",j_par),ylabel="freq" )
            display(p2)
        end     # end of parameters loop
    end     # end if withgraphical
    # ...control-window:
    if( nutsopt.without>=1 )
        jj_par = 0                  # initialise
        @printf( " (%s) Info - nutssampler: Final statistics after burnin (after %1.3f sec):\n", nutsopt.name, (DateTime(now())-t1)/Millisecond(1000) )
        for j_par = collect(1:nutsopt.noparams)[nutsopt.keephistoryof]
            jj_par += 1             # go through entries of x_hist
            @printf( " (%s) par(%2d): %+1.5e +- %1.5e\n", nutsopt.name, j_par, mean(x_hist[jj_par,statsrange]),std(x_hist[jj_par,statsrange]) )
        end     # end of parameters loop
        @printf( " (%s)  mean rejection rate: %1.3e for timestep %1.3e\n", nutsopt.name, nutsopt.rejrate , nutsopt.timestep )
        @printf( " (%s) Info - nutssampler: Total time-consumption %1.3f sec.\n", nutsopt.name, (DateTime(now())-t1)/Millisecond(1000) )
    end     # end if without

    return x_hist[:,statsrange]
end   # end of nutssampler function

function getlogtarget_nuts( logtarget::Function, x::Array{Float64,1}, nutsopt::Nutsoptions )
    # gives logtarget at position x
    #@printf( " Start getlogtarget_nuts: x = [ %s ]\n", getstringfromvector(x) )

    return logtarget(x)
end     # end of getlogtarget_nuts function
function getgradient_nuts( logtarget::Function,gradient::Function, x::Array{Float64,1}, nutsopt::Nutsoptions )
    # estimates gradient of logtarget at position x

    #@printf( " Start getgrad_nuts: x = [ %s ]\n", getstringfromvector(x) )
    if( nutsopt.approxgradient )        # need to approximate via Euler approximation
        logtarget_x = getlogtarget_nuts( logtarget,x,nutsopt )
        gradx = zeros(nutsopt.noparams) # initialise
        x_pert = zeros(nutsopt.noparams)
        for j_par = 1:nutsopt.noparams
            x_pert = deepcopy(x);   x_pert[j_par] += nutsopt.dx[j_par]
            gradx[j_par] = (getlogtarget_nuts( logtarget,x_pert,nutsopt ) - logtarget_x)/nutsopt.dx[j_par]
        end     # end of parameters loop
        #@printf( " Info - getgradient_nuts: logtarget_x = %+1.5e, grad_x = [ %s ], dx = [ %s ]\n", logtarget_x,getstringfromvector(gradx),getstringfromvector(nutsopt.dx) )
        #display( sum(gradx.==0) )
    else                                # can use gradient function
        gradx = gradient(x)
    end     # end if gradient gets estimated numerically

    return gradx
end     # end of getgradient_nuts function
function leapfrogstep( logtarget::Function,gradient::Function, x::Array{Float64,1},p::Array{Float64,1}, direction::Int, nutsopt::Nutsoptions )
    # calculates leapfrog-update for (x,p)
    # direction is +-1 for forward/backward in time evolutions

    x_evol = deepcopy(x);   p_evol = deepcopy(p)
    p_evol += getgradient_nuts( logtarget,gradient,x_evol,nutsopt ).*(direction*nutsopt.timestep/2)
    x_evol += p_evol.*(direction*nutsopt.timestep/nutsopt.mass)
    p_evol += getgradient_nuts( logtarget,gradient,x_evol,nutsopt ).*(direction*nutsopt.timestep/2)

    if( !all(.!isnan.(x_evol)) | !all(.!isnan.(p_evol)) )   # something went wrong
        prblmflag = true                                    # report problem
        if( nutsopt.without>=3 )
            @printf( " (%s) Info - leapfrogstep: Got pathological evolution:\n", nutsopt.name )
            @printf( " (%s)  x       = [ %s ]\n", nutsopt.name, getstringfromvector(x) )
            @printf( " (%s)  p       = [ %s ]\n", nutsopt.name, getstringfromvector(p) )
            @printf( " (%s)  x_evol  = [ %s ]\n", nutsopt.name, getstringfromvector(x_evol) )
            @printf( " (%s)  p_evol  = [ %s ]\n", nutsopt.name, getstringfromvector(p_evol) )
            @printf( " (%s)  1stgrad = [ %s ]\n", nutsopt.name, getstringfromvector( getgradient_nuts( logtarget,gradient,x,nutsopt ) ) )
            @printf( " (%s)  2ndgrad = [ %s ]\n", nutsopt.name, getstringfromvector( getgradient_nuts( logtarget,gradient,x_evol,nutsopt ) ) )
        end     # end if without
    else                                                    # no problem to report
        prblmflag = false
    end     # end if pathological

    return x_evol,p_evol, prblmflag, nutsopt
end     # end of leapfrogstep function
function gettreebranch( logtarget::Function,gradient::Function, x_curr::Array{Float64,1},p_curr::Array{Float64,1},logu_curr::Float64, jointlogtarget_curr::Float64, direction::Int, treedepth::UInt64, nutsopt::Nutsoptions )
    # generates one more branch of given depth to the binary tree

    if( nutsopt.without>=3 )
        @printf( " (%s) Info - gettreebranch: Branch off at x=[ %s ] in direction %d.\n", nutsopt.name, getstringfromvector(x_curr), direction )
    end     # end if without
    if( treedepth==0 )                          # lowest/initial branch
        (x_prop,p_prop, prblmflag, nutsopt) = leapfrogstep( logtarget,gradient, x_curr,p_curr, direction, nutsopt )
        jointlogtarget_prop = getlogtarget_nuts( logtarget,x_prop,nutsopt ) + (-1/2)*sum( p_prop.^2 )
        weight_prop = UInt64(logu_curr<=jointlogtarget_prop)
        keepongoing_prop = (logu_curr<(jointlogtarget_prop+nutsopt.Delta_max))
        x_early = deepcopy(x_prop);     x_late = deepcopy(x_prop)   # earliest and latest positions
        p_early = deepcopy(p_prop);     p_late = deepcopy(p_prop)   # earliest and latest momenta
        if( !prblmflag )                        # no problem reported
            nutsopt.alpha = min( exp( jointlogtarget_prop - jointlogtarget_curr ), 1.0 )
        else                                    # problem reported in leapfrogstep
            nutsopt.alpha = 0.0
        end     # end if problemflag raised
        nutsopt.n_alpha = UInt64(1)
    else                                        # still have to work on larger depth
        (x_early,p_early, x_late,p_late, x_prop,p_prop, jointlogtarget_prop, weight_prop,keepongoing_prop, nutsopt) = gettreebranch( logtarget,gradient, x_curr,p_curr,logu_curr, jointlogtarget_curr, direction, treedepth-1, nutsopt )    # first half of new part
        oldalpha = deepcopy(nutsopt.alpha);     oldn_alpha = deepcopy(nutsopt.n_alpha)  # remember
        if( keepongoing_prop )
            if( direction>0 )                   # forward in time
                ( _, _, x_late,p_late, x_propprop,p_propprop, jointlogtarget_propprop, weight_propprop, keepongoing_prop, nutsopt_here ) = gettreebranch( logtarget,gradient, x_late,p_late,logu_curr, jointlogtarget_curr, direction, treedepth-1, nutsopt )     # second half of new part
            else                                # backward in time
                ( x_early,p_early, _, _, x_propprop,p_propprop, jointlogtarget_propprop, weight_propprop, keepongoing_prop, nutsopt_here ) = gettreebranch( logtarget,gradient, x_early,p_early,logu_curr, jointlogtarget_curr, direction, treedepth-1, nutsopt ) # second half of new part
            end     # end of distinguishing time-direction
            if( rand()<(weight_propprop/(weight_prop+weight_propprop)) )    # choose sample from each half from each half-tree with equal probability
                x_prop = deepcopy(x_propprop);  p_prop = deepcopy(p_propprop); jointlogtarget_prop = deepcopy(jointlogtarget_propprop)
            end     # end if choose from second half-tree instead
            weight_prop += weight_propprop      # combine weights when merging
            uturncrit = ( sum((x_late-x_early).*p_early)>=0 );  keepongoing_prop = ( keepongoing_prop & uturncrit )
            uturncrit = ( sum((x_late-x_early).*p_late)>=0 );   keepongoing_prop = ( keepongoing_prop & uturncrit )
            nutsopt.alpha = oldalpha + nutsopt_here.alpha
            nutsopt.n_alpha = oldn_alpha + nutsopt_here.n_alpha
        else                                    # ie don't keepongoing: new half not adopted, but old half still contributes to statistic of timestep
            nutsopt.alpha = deepcopy( oldalpha )
            nutsopt.n_alpha = deepcopy( oldn_alpha )
        end     # end if keepongoing
    end      # end if zero treedepth

    return x_early,p_early, x_late,p_late, x_prop,p_prop, jointlogtarget_prop, weight_prop,keepongoing_prop, nutsopt
end     # end of gettreebranch function
function singlenutsupdate( logtarget::Function,gradient::Function, x_curr::Array{Float64,1}, nutsopt::Nutsoptions )
    # one new nuts sample

    # sample momentun p and slice-variable u:
    p_curr = randn(nutsopt.noparams)./nutsopt.mass
    jointlogtarget_curr = getlogtarget_nuts( logtarget,x_curr,nutsopt ) + (-1/2)*sum( p_curr.^2 )
    logu_curr = log(rand()) + jointlogtarget_curr
    x_early = deepcopy(x_curr);     x_late = deepcopy(x_curr)   # earliest and latest positions
    p_early = deepcopy(p_curr);     p_late = deepcopy(p_curr)   # earliest and latest momenta
    treedepth = UInt64(0)                                       # initialise at base level
    weight_curr = UInt64(1)                                     # number of nodes in slice-support
    keepongoing = true                                          # reset stopping criterion
    while( keepongoing )
        direction = rand([-1,+1])                               # +-1 with same probability
        if( nutsopt.without>=3 )
            @printf( " (%s) Info - singlenutsupdate: Start branch from x=[ %s ],p=[ %s ],logu=%1.5e in direction %d.\n", nutsopt.name, getstringfromvector(x_curr),getstringfromvector(p_curr),logu_curr, direction )
            @printf( " (%s)  ...early: x=[ %s ],p=[ %s ]\n", nutsopt.name, getstringfromvector(x_early),getstringfromvector(p_early) )
            @printf( " (%s)  ...late : x=[ %s ],p=[ %s ]\n", nutsopt.name, getstringfromvector(x_late), getstringfromvector(p_late)  )
        end     # end if without
        if( direction>0 )                                       # forward in time
            ( _, _, x_late,p_late, x_prop, _, _, weight_prop, keepongoing, nutsopt ) = gettreebranch( logtarget,gradient, x_late,p_late,logu_curr, jointlogtarget_curr, direction, treedepth, nutsopt )
        else                                                    # backward in time
            ( x_early,p_early, _, _, x_prop, _, _, weight_prop, keepongoing, nutsopt ) = gettreebranch( logtarget,gradient, x_early,p_early,logu_curr, jointlogtarget_curr, direction, treedepth, nutsopt )
        end     # end of deciding direction
        if( keepongoing )                                       # if no hard boundary yet
            if( rand()<(weight_prop/weight_curr) )
                x_curr = deepcopy( x_prop )                     # otherwise keep sample from previous tree
            end     # end if accept
        end     # end if keepongoing
        weight_curr += weight_prop                              # combine weights when merging
        uturncrit = ( sum((x_late-x_early).*p_early)>=0 );  keepongoing = ( keepongoing & uturncrit )
        uturncrit = ( sum((x_late-x_early).*p_late)>=0 );   keepongoing = ( keepongoing & uturncrit )
        treedepth += 1                                          # got one depth deeper
    end     # end while keepongoing

    return x_curr, nutsopt
end     # end of singlenutsupdate function
function adjusttimestep( j_MC::UInt64, nutsopt::Nutsoptions )
    # adjusts stepsizes to get approximately the desired acceptance rate

    if( nutsopt.adjusttimestep & (j_MC<=nutsopt.burnin) ) # adjust timestep
        lambda = 1/( j_MC + nutsopt.timeadj_t0 )
        nutsopt.timeadj_Hb = (1-lambda)*nutsopt.timeadj_Hb + lambda*(nutsopt.timeadj_reasonableacceptancerate - (nutsopt.alpha/nutsopt.n_alpha))
        nutsopt.timestep = exp( nutsopt.timeadj_mu-nutsopt.timeadj_Hb*sqrt(j_MC)/nutsopt.timeadj_gamma )
        lambda = j_MC^(-nutsopt.timeadj_kappa)
        nutsopt.timeadj_timestepb = exp( lambda*log(nutsopt.timestep) + (1-lambda)*log(nutsopt.timeadj_timestepb) )
    end     # end if stepsize adjustment suppressed
    return nutsopt
end     # end of adjusttimestep function

function getstringfromvector( myvector::Union{Array{Float64,1},Array{Int64,1},Array{UInt,1},Array{Bool,1}} )
    # outputs string of vector elements
    if( !isempty(myvector) )
        if( typeof(myvector)<:Array{Float64} )
            myword = @sprintf( "%+12.5e", myvector[1] )
            for j = 2:length(myvector)
                myword = @sprintf( "%s %+12.5e", myword, myvector[j] )
            end     # end of vectorelements loop
        elseif( typeof(myvector)<:Array{Int64} )
            myword = @sprintf( "%+12d", myvector[1] )
            for j = 2:length(myvector)
                myword = @sprintf( "%s %+12d", myword, myvector[j] )
            end     # end of vectorelements loop
        elseif( typeof(myvector)<:Array{UInt} )
            myword = @sprintf( "%12d", myvector[1] )
            for j = 2:length(myvector)
                myword = @sprintf( "%s %12d", myword, myvector[j] )
            end     # end of vectorelements loop
        elseif( typeof(myvector)<:Array{Bool} )
            myword = @sprintf( "%12d", myvector[1] )
            for j = 2:length(myvector)
                myword = @sprintf( "%s %12d", myword, myvector[j] )
            end     # end of vectorelements loop
        else    # unknown type
            @printf( " Warning - getstringfromvector: Unsupported type: %s.\n", typeof(myvector) )
            display( myvector )
        end     # end of distinguishing
    else    # empty input vector
        myword = ""
    end     # end if empty input vector
    return myword
end     # end of getstringfromvector function
