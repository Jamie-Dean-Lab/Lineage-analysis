using Printf
using Dates
using Plots

Base.@kwdef mutable struct Nutsoptions
	# options for nuts_sampler

	autogenerated::Bool = true                      # indicate if autogenerated
	name::String = ""                               # name/comment
	noparams::UInt64 = 1                            # size of parameter space
	without::UInt64 = 1                             # '0' for only warnings, '3' for debugging
	withgraphical::Bool = false                     # 'true' for graphical output in the end, 'false' otherwise
	burnin::UInt64 = 10000
	MCmax::UInt64 = 2 * burnin
	subsamplefreq::UInt64 = 1                       # number of subsamples per recorded sample
	approxgradient::Bool = true                     # 'true', if use Euler approximation for gradient estimation, 'false' for using gradient function
	dx::Array{Float64, 1} = ones(noparams) .* (1E-7)   # should be positive
	timestep::Float64 = 1E-2                        # should be positive
	mass::Float64 = 1.0                             # should be positive
	Delta_max::Float64 = 1E3                        # should be non-negative and large
	keephistoryof::Array{Bool, 1} = trues(noparams)  # 'true', if respective parameter should be recorded, 'false' otherwise
	alpha::Float64 = 0.0
	n_alpha::UInt64 = 0
	# stepsize adjustment:
	adjusttimestep::Bool = true
	timeadj_reasonableacceptancerate::Float64 = 0.65
	timeadj_Hb::Float64 = 0.0                       # measures deviations from desired rejection rate
	timeadj_t0::Float64 = 10.0                      # suppress early iterations
	timeadj_gamma::Float64 = 0.05                   # scaling for the penalty - larger changes for smaller gamma
	timeadj_kappa::Float64 = 0.75                   # exponent, how quickly timestepcorrection changes fade out in the MC evolution
	timeadj_mu::Float64 = log(10 * timestep)          # offset for timestep
	timeadj_timestepb::Float64 = 1.0
	rejrate::Float64 = 0.0                          # rejection rate
end     # end of Nutsoptions struct

function nuts_sampler(logtarget::Function, gradient::Function, x_init::Array{Float64, 1}, nutsopt::Nutsoptions)
	# NUTS sampler

	t1 = DateTime(now())
	# initialise nutsopt:
	if (nutsopt.autogenerated)                     # if input is autogenerated
		noparams = length(x_init)
		dx = ones(noparams) * nutsopt.dx[1]
		keephistoryof = trues(noparams)
		nutsopt.noparams = noparams
		nutsopt.dx = dx
		nutsopt.keephistoryof = keephistoryof   # alpha, n_alpha will get overwritten at tree of depth 0
	end     # end if nutsoptions not given
	x_curr = deepcopy(x_init)                     # start location
	logtarget_curr = get_log_target_nuts(logtarget, x_init, nutsopt)
	gradx = get_gradient_nuts(logtarget, gradient, x_init, nutsopt)
	if (isinf(logtarget_curr) | isnan(logtarget_curr) | !all(.!isinf.(gradx)) | !all(.!isnan.(gradx)))
		@printf(" (%s) Warning - nuts_sampler: Pathological initial condition, logtarget = %+1.5e, gradx = [ %s ], x_init = [ %s ].\n", nutsopt.name, logtarget_curr, get_string_from_vector(gradx), get_string_from_vector(x_init))
	end     # end if pathological initial condition
	nextprogessinfo = 1                 # first information bit to be next
	x_hist = zeros(sum(nutsopt.keephistoryof), nutsopt.MCmax + 1)
	x_hist[:, 1] = x_curr[nutsopt.keephistoryof] # initialise history
	accrate_hist = zeros(nutsopt.MCmax + 1)
	for j_MC ∈ 2:(nutsopt.MCmax+1)
		if ((nutsopt.without >= 1) & ((nextprogessinfo * 0.1) <= (j_MC / (nutsopt.MCmax + 1))))
			@printf(" (%s) Info - nuts_sampler: Start iteration %d now (after %1.3f sec).\n", nutsopt.name, j_MC, (DateTime(now()) - t1) / Millisecond(1000))
			nextprogessinfo += 1        # progess to next step of information
		end     # end if time for output
		for j_sumbsample ∈ 1:nutsopt.subsamplefreq
			(x_curr, nutsopt) = single_nuts_update(logtarget, gradient, x_curr, nutsopt)
			x_hist[:, j_MC] = x_curr[nutsopt.keephistoryof]
		end     # end of subsample loop
		accrate_hist[j_MC] = nutsopt.alpha / nutsopt.n_alpha
		nutsopt = adjust_timestep(j_MC, nutsopt)
	end     # end of MC iterations
	statsrange = (nutsopt.burnin+1):(nutsopt.MCmax+1)           # post-burnin samples for statistical evaluation
	nutsopt.rejrate = 1 - mean(accrate_hist[statsrange])        # mean rejection rate

	# output:
	# ...graphical:
	if (nutsopt.withgraphical)
		res = Int64(ceil(2 * ((length(statsrange))^(1 / 3))))
		jj_par = 0                  # initialise
		for j_par ∈ collect(1:nutsopt.noparams)[nutsopt.keephistoryof]
			jj_par += 1
			p1 = plot(1:(nutsopt.MCmax+1), x_hist[jj_par, :], title = @sprintf("(%s) par %d evolution", nutsopt.name, j_par), xlabel = "MCit", ylabel = @sprintf("par %d", j_par))
			display(p1)
			p2 = histogram(x_hist[jj_par, statsrange], nbins = res, title = @sprintf("(%s) par %d histogram after burnin", nutsopt.name, j_par), xlabel = @sprintf("par %d", j_par), ylabel = "freq")
			display(p2)
		end     # end of parameters loop
	end     # end if withgraphical
	# ...control-window:
	if (nutsopt.without >= 1)
		jj_par = 0                  # initialise
		@printf(" (%s) Info - nuts_sampler: Final statistics after burnin (after %1.3f sec):\n", nutsopt.name, (DateTime(now()) - t1) / Millisecond(1000))
		for j_par ∈ collect(1:nutsopt.noparams)[nutsopt.keephistoryof]
			jj_par += 1             # go through entries of x_hist
			@printf(" (%s) par(%2d): %+1.5e +- %1.5e\n", nutsopt.name, j_par, mean(x_hist[jj_par, statsrange]), std(x_hist[jj_par, statsrange]))
		end     # end of parameters loop
		@printf(" (%s)  mean rejection rate: %1.3e for timestep %1.3e\n", nutsopt.name, nutsopt.rejrate, nutsopt.timestep)
		@printf(" (%s) Info - nuts_sampler: Total time-consumption %1.3f sec.\n", nutsopt.name, (DateTime(now()) - t1) / Millisecond(1000))
	end     # end if without

	return x_hist[:, statsrange]
end   # end of nuts_sampler function

function get_log_target_nuts(logtarget::Function, x::Array{Float64, 1}, nutsopt::Nutsoptions)
	# gives logtarget at position x
	#@printf( " Start get_log_target_nuts: x = [ %s ]\n", get_string_from_vector(x) )

	return logtarget(x)
end     # end of get_log_target_nuts function

function get_gradient_nuts(logtarget::Function, gradient::Function, x::Array{Float64, 1}, nutsopt::Nutsoptions)
	# estimates gradient of logtarget at position x

	#@printf( " Start getgrad_nuts: x = [ %s ]\n", get_string_from_vector(x) )
	if (nutsopt.approxgradient)        # need to approximate via Euler approximation
		logtarget_x = get_log_target_nuts(logtarget, x, nutsopt)
		gradx = zeros(nutsopt.noparams) # initialise
		x_pert = zeros(nutsopt.noparams)
		for j_par ∈ 1:nutsopt.noparams
			x_pert = deepcopy(x)
			x_pert[j_par] += nutsopt.dx[j_par]
			gradx[j_par] = (get_log_target_nuts(logtarget, x_pert, nutsopt) - logtarget_x) / nutsopt.dx[j_par]
		end     # end of parameters loop
	#@printf( " Info - get_gradient_nuts: logtarget_x = %+1.5e, grad_x = [ %s ], dx = [ %s ]\n", logtarget_x,get_string_from_vector(gradx),get_string_from_vector(nutsopt.dx) )
	#display( sum(gradx.==0) )
	else                                # can use gradient function
		gradx = gradient(x)
	end     # end if gradient gets estimated numerically

	return gradx
end     # end of get_gradient_nuts function

function leapfrog_step(logtarget::Function, gradient::Function, x::Array{Float64, 1}, p::Array{Float64, 1}, direction::Int, nutsopt::Nutsoptions)
	# calculates leapfrog-update for (x,p)
	# direction is +-1 for forward/backward in time evolutions

	x_evol = deepcopy(x)
	p_evol = deepcopy(p)
	p_evol += get_gradient_nuts(logtarget, gradient, x_evol, nutsopt) .* (direction * nutsopt.timestep / 2)
	x_evol += p_evol .* (direction * nutsopt.timestep / nutsopt.mass)
	p_evol += get_gradient_nuts(logtarget, gradient, x_evol, nutsopt) .* (direction * nutsopt.timestep / 2)

	if (!all(.!isnan.(x_evol)) | !all(.!isnan.(p_evol)))   # something went wrong
		prblmflag = true                                    # report problem
		if (nutsopt.without >= 3)
			@printf(" (%s) Info - leapfrog_step: Got pathological evolution:\n", nutsopt.name)
			@printf(" (%s)  x       = [ %s ]\n", nutsopt.name, get_string_from_vector(x))
			@printf(" (%s)  p       = [ %s ]\n", nutsopt.name, get_string_from_vector(p))
			@printf(" (%s)  x_evol  = [ %s ]\n", nutsopt.name, get_string_from_vector(x_evol))
			@printf(" (%s)  p_evol  = [ %s ]\n", nutsopt.name, get_string_from_vector(p_evol))
			@printf(" (%s)  1stgrad = [ %s ]\n", nutsopt.name, get_string_from_vector(get_gradient_nuts(logtarget, gradient, x, nutsopt)))
			@printf(" (%s)  2ndgrad = [ %s ]\n", nutsopt.name, get_string_from_vector(get_gradient_nuts(logtarget, gradient, x_evol, nutsopt)))
		end     # end if without
	else                                                    # no problem to report
		prblmflag = false
	end     # end if pathological

	return x_evol, p_evol, prblmflag, nutsopt
end     # end of leapfrog_step function

function get_tree_branch(logtarget::Function, gradient::Function, x_curr::Array{Float64, 1}, p_curr::Array{Float64, 1}, logu_curr::Float64, jointlogtarget_curr::Float64, direction::Int, treedepth::UInt64, nutsopt::Nutsoptions)
	# generates one more branch of given depth to the binary tree

	if (nutsopt.without >= 3)
		@printf(" (%s) Info - get_tree_branch: Branch off at x=[ %s ] in direction %d.\n", nutsopt.name, get_string_from_vector(x_curr), direction)
	end     # end if without
	if (treedepth == 0)                          # lowest/initial branch
		(x_prop, p_prop, prblmflag, nutsopt) = leapfrog_step(logtarget, gradient, x_curr, p_curr, direction, nutsopt)
		jointlogtarget_prop = get_log_target_nuts(logtarget, x_prop, nutsopt) + (-1 / 2) * sum(p_prop .^ 2)
		weight_prop = UInt64(logu_curr <= jointlogtarget_prop)
		keepongoing_prop = (logu_curr < (jointlogtarget_prop + nutsopt.Delta_max))
		x_early = deepcopy(x_prop)
		x_late = deepcopy(x_prop)   # earliest and latest positions
		p_early = deepcopy(p_prop)
		p_late = deepcopy(p_prop)   # earliest and latest momenta
		if (!prblmflag)                        # no problem reported
			nutsopt.alpha = min(exp(jointlogtarget_prop - jointlogtarget_curr), 1.0)
		else                                    # problem reported in leapfrog_step
			nutsopt.alpha = 0.0
		end     # end if problemflag raised
		nutsopt.n_alpha = UInt64(1)
	else                                        # still have to work on larger depth
		(x_early, p_early, x_late, p_late, x_prop, p_prop, jointlogtarget_prop, weight_prop, keepongoing_prop, nutsopt) = get_tree_branch(logtarget, gradient, x_curr, p_curr, logu_curr, jointlogtarget_curr, direction, treedepth - 1, nutsopt)    # first half of new part
		oldalpha = deepcopy(nutsopt.alpha)
		oldn_alpha = deepcopy(nutsopt.n_alpha)  # remember
		if (keepongoing_prop)
			if (direction > 0)                   # forward in time
				(_, _, x_late, p_late, x_propprop, p_propprop, jointlogtarget_propprop, weight_propprop, keepongoing_prop, nutsopt_here) =
					get_tree_branch(logtarget, gradient, x_late, p_late, logu_curr, jointlogtarget_curr, direction, treedepth - 1, nutsopt)     # second half of new part
			else                                # backward in time
				(x_early, p_early, _, _, x_propprop, p_propprop, jointlogtarget_propprop, weight_propprop, keepongoing_prop, nutsopt_here) =
					get_tree_branch(logtarget, gradient, x_early, p_early, logu_curr, jointlogtarget_curr, direction, treedepth - 1, nutsopt) # second half of new part
			end     # end of distinguishing time-direction
			if (rand() < (weight_propprop / (weight_prop + weight_propprop)))    # choose sample from each half from each half-tree with equal probability
				x_prop = deepcopy(x_propprop)
				p_prop = deepcopy(p_propprop)
				jointlogtarget_prop = deepcopy(jointlogtarget_propprop)
			end     # end if choose from second half-tree instead
			weight_prop += weight_propprop      # combine weights when merging
			uturncrit = (sum((x_late - x_early) .* p_early) >= 0)
			keepongoing_prop = (keepongoing_prop & uturncrit)
			uturncrit = (sum((x_late - x_early) .* p_late) >= 0)
			keepongoing_prop = (keepongoing_prop & uturncrit)
			nutsopt.alpha = oldalpha + nutsopt_here.alpha
			nutsopt.n_alpha = oldn_alpha + nutsopt_here.n_alpha
		else                                    # ie don't keepongoing: new half not adopted, but old half still contributes to statistic of timestep
			nutsopt.alpha = deepcopy(oldalpha)
			nutsopt.n_alpha = deepcopy(oldn_alpha)
		end     # end if keepongoing
	end      # end if zero treedepth

	return x_early, p_early, x_late, p_late, x_prop, p_prop, jointlogtarget_prop, weight_prop, keepongoing_prop, nutsopt
end     # end of get_tree_branch function

function single_nuts_update(logtarget::Function, gradient::Function, x_curr::Array{Float64, 1}, nutsopt::Nutsoptions)
	# one new nuts sample

	# sample momentun p and slice-variable u:
	p_curr = randn(nutsopt.noparams) ./ nutsopt.mass
	jointlogtarget_curr = get_log_target_nuts(logtarget, x_curr, nutsopt) + (-1 / 2) * sum(p_curr .^ 2)
	logu_curr = log(rand()) + jointlogtarget_curr
	x_early = deepcopy(x_curr)
	x_late = deepcopy(x_curr)   # earliest and latest positions
	p_early = deepcopy(p_curr)
	p_late = deepcopy(p_curr)   # earliest and latest momenta
	treedepth = UInt64(0)                                       # initialise at base level
	weight_curr = UInt64(1)                                     # number of nodes in slice-support
	keepongoing = true                                          # reset stopping criterion
	while (keepongoing)
		direction = rand([-1, +1])                               # +-1 with same probability
		if (nutsopt.without >= 3)
			@printf(" (%s) Info - single_nuts_update: Start branch from x=[ %s ],p=[ %s ],logu=%1.5e in direction %d.\n", nutsopt.name, get_string_from_vector(x_curr), get_string_from_vector(p_curr), logu_curr, direction)
			@printf(" (%s)  ...early: x=[ %s ],p=[ %s ]\n", nutsopt.name, get_string_from_vector(x_early), get_string_from_vector(p_early))
			@printf(" (%s)  ...late : x=[ %s ],p=[ %s ]\n", nutsopt.name, get_string_from_vector(x_late), get_string_from_vector(p_late))
		end     # end if without
		if (direction > 0)                                       # forward in time
			(_, _, x_late, p_late, x_prop, _, _, weight_prop, keepongoing, nutsopt) = get_tree_branch(logtarget, gradient, x_late, p_late, logu_curr, jointlogtarget_curr, direction, treedepth, nutsopt)
		else                                                    # backward in time
			(x_early, p_early, _, _, x_prop, _, _, weight_prop, keepongoing, nutsopt) = get_tree_branch(logtarget, gradient, x_early, p_early, logu_curr, jointlogtarget_curr, direction, treedepth, nutsopt)
		end     # end of deciding direction
		if (keepongoing)                                       # if no hard boundary yet
			if (rand() < (weight_prop / weight_curr))
				x_curr = deepcopy(x_prop)                     # otherwise keep sample from previous tree
			end     # end if accept
		end     # end if keepongoing
		weight_curr += weight_prop                              # combine weights when merging
		uturncrit = (sum((x_late - x_early) .* p_early) >= 0)
		keepongoing = (keepongoing & uturncrit)
		uturncrit = (sum((x_late - x_early) .* p_late) >= 0)
		keepongoing = (keepongoing & uturncrit)
		treedepth += 1                                          # got one depth deeper
	end     # end while keepongoing

	return x_curr, nutsopt
end     # end of single_nuts_update function

function adjust_timestep(j_MC::UInt64, nutsopt::Nutsoptions)
	# adjusts stepsizes to get approximately the desired acceptance rate

	if (nutsopt.adjusttimestep & (j_MC <= nutsopt.burnin)) # adjust timestep
		lambda = 1 / (j_MC + nutsopt.timeadj_t0)
		nutsopt.timeadj_Hb = (1 - lambda) * nutsopt.timeadj_Hb + lambda * (nutsopt.timeadj_reasonableacceptancerate - (nutsopt.alpha / nutsopt.n_alpha))
		nutsopt.timestep = exp(nutsopt.timeadj_mu - nutsopt.timeadj_Hb * sqrt(j_MC) / nutsopt.timeadj_gamma)
		lambda = j_MC^(-nutsopt.timeadj_kappa)
		nutsopt.timeadj_timestepb = exp(lambda * log(nutsopt.timestep) + (1 - lambda) * log(nutsopt.timeadj_timestepb))
	end     # end if stepsize adjustment suppressed
	return nutsopt
end     # end of adjust_timestep function

function get_string_from_vector(myvector::Union{Array{Float64, 1}, Array{Int64, 1}, Array{UInt, 1}, Array{Bool, 1}})
	# outputs string of vector elements

	if (!isempty(myvector))
		if (typeof(myvector) <: Array{Float64})
			myword = @sprintf("%+12.5e", myvector[1])
			for j ∈ 2:length(myvector)
				myword = @sprintf("%s %+12.5e", myword, myvector[j])
			end     # end of vectorelements loop
		elseif (typeof(myvector) <: Array{Int64})
			myword = @sprintf("%+12d", myvector[1])
			for j ∈ 2:length(myvector)
				myword = @sprintf("%s %+12d", myword, myvector[j])
			end     # end of vectorelements loop
		elseif (typeof(myvector) <: Array{UInt})
			myword = @sprintf("%12d", myvector[1])
			for j ∈ 2:length(myvector)
				myword = @sprintf("%s %12d", myword, myvector[j])
			end     # end of vectorelements loop
		elseif (typeof(myvector) <: Array{Bool})
			myword = @sprintf("%12d", myvector[1])
			for j ∈ 2:length(myvector)
				myword = @sprintf("%s %12d", myword, myvector[j])
			end     # end of vectorelements loop
		else    # unknown type
			@printf(" Warning - get_string_from_vector: Unsupported type: %s.\n", typeof(myvector))
			display(myvector)
		end     # end of distinguishing
	else    # empty input vector
		myword = ""
	end     # end if empty input vector
	return myword
end     # end of get_string_from_vector function
